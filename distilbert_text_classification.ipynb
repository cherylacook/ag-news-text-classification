{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c3d917-a00c-4718-a7ba-24dd9d71ed25",
   "metadata": {},
   "source": [
    "# DistilBERT for Text Classification\n",
    "\n",
    "Objective: To fine-tune a pre-trained DistilBERT transformer for text classification on the AG News dataset, and evaluate its performance against the traditional and CNN-based approaches (from 'traditional_vs_cnn_text_classification.ipynb'. The goal is to assess the benefit of large pre-trained language models for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af46e42-a88b-4643-96d5-c0320bd3e8b1",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "\n",
    "This cell imports all libraries necessary for data pre-processing, model setup, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Qd6wfZvKkJuY",
   "metadata": {
    "id": "Qd6wfZvKkJuY"
   },
   "outputs": [],
   "source": [
    "# Checking for the libraries that don't come pre-installed with Python or Anaconda, and installing them if needed\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip install torch\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    !pip install transformers\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "except ImportError:\n",
    "    !pip install datasets\n",
    "\n",
    "# Checking that the rest of the necessary libraries import properly \n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9656b26-0b3c-49b4-8f6a-f1e4b4250417",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning\n",
    "\n",
    "The AG News dataset is loaded from the provided train.csv and test.csv files. The target labels are encoded numerically, and each article's title and description are concatenated to form a single input sequence. The text is lowercased and punctuation/special characters are removed to reduce noise and standardise inputs for DistilBERT tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47261a61-65b5-49e9-b737-8b5f89726979",
   "metadata": {
    "id": "47261a61-65b5-49e9-b737-8b5f89726979"
   },
   "outputs": [],
   "source": [
    "# Importing training and testing datasets\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Data pre-processing\n",
    "\n",
    "# Label encoding the target 'Class Index' variable\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_data['Class Index'])\n",
    "test_y = encoder.transform(test_data['Class Index'])\n",
    "\n",
    "# Combining the Title and Description as the text input for each instance\n",
    "train_x = train_data['Title'] + ' ' + train_data['Description']\n",
    "test_x = test_data['Title'] + ' ' + test_data['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_-75SNOkPx5d",
   "metadata": {
    "id": "_-75SNOkPx5d"
   },
   "outputs": [],
   "source": [
    "import re # used for removing certain characters from the dataset\n",
    "\n",
    "# Data preprocessing / cleaning\n",
    "\n",
    "# Splitting the training set into a smaller training set and a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.25, random_state=42, shuffle=True) # test_size=0.25 is default setting\n",
    "\n",
    "# Cleaning up data by making it all lowercase and removing certain characters\n",
    "def text_cleaner(og_text):\n",
    "  clean_text = og_text.lower() # Converting to lowercase for consistency\n",
    "  clean_text = re.sub(r'[^a-z0-9\\s]', '', clean_text) # eliminates punctuation and special characters\n",
    "  return clean_text\n",
    "\n",
    "clean_train_x = train_x.apply(text_cleaner)\n",
    "clean_val_x = val_x.apply(text_cleaner)\n",
    "clean_test_x = test_x.apply(text_cleaner) # Preprocessing also applies to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0dcb9-260b-4a45-ae79-4428d3546716",
   "metadata": {},
   "source": [
    "## 3. Tokenisation with pre-trained DistilBERT\n",
    "\n",
    "The pre-trained DistilBERT tokenizer is loaded to convert textual input into token IDs suitable for the model.\n",
    "The cleaned training, validation, and test sets are first stored in Pandas DataFrames, then converted to HuggingFace 'Dataset' objects.\n",
    "Finally, each dataset is tokenised with truncation and padding to a maximum sequence length of 128 tokens, ensurig consistent input size for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LHuW2kJsP06Q",
   "metadata": {
    "id": "LHuW2kJsP06Q"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb867a039a2d4de18f546a7986dde146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b0ab8967ea43b8a64b58c9eba892b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22074f805b1a4fce981c3371bb9214c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the pre-trained DistilBERT tokenizer and tokenizing the data\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42) # for consistency\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Storing the cleaned data in Pandas DataFrames\n",
    "train_df = pd.DataFrame({'clean_text': clean_train_x, 'label': train_y})\n",
    "val_df = pd.DataFrame({'clean_text': clean_val_x, 'label': val_y})\n",
    "test_df = pd.DataFrame({'clean_text': clean_test_x, 'label': test_y})\n",
    "\n",
    "# Converting the DataFrames to a HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "def data_tokenizer(data):\n",
    "  return tokenizer(data['clean_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(data_tokenizer, batched=True)\n",
    "tokenized_val = val_dataset.map(data_tokenizer, batched=True)\n",
    "tokenized_test = test_dataset.map(data_tokenizer, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22164f-de3a-41ac-ab7c-a6dea416eda7",
   "metadata": {},
   "source": [
    "## 4. Setting up and Training the DistilBERT Model\n",
    "The pre-trained DistilBERT model is loaded for sequence classification with 4 output labels for the 4 AG dataset classes. TrainingArguments are configured for 10 epochs, a learning rate of 2e-5, batch size 32, and early stopping if validation metrics do not improve for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "VtN_FO3AQA4g",
   "metadata": {
    "id": "VtN_FO3AQA4g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Setting up a Trainer with the pretrained DistilBERT model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "n_classes = 4 # AG dataset has 4 class labels\n",
    "lr = 2e-5 # standard learning rate for fine-tuning DistilBERT\n",
    "batch_size = 32 # fits comfortably on GPU memory\n",
    "train_epochs = 10 # sufficient to see convergence on validation set\n",
    "\n",
    "# Loading the pre-trained DistilBERT model to use for sequence classification\n",
    "distilbert_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=n_classes)\n",
    "\n",
    "# Defining the arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', # saves the model's checkpoints, logs, and final results\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch', # evaluates the model on the validation set every epoch\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_strategy='epoch', # saves the model's checkpoints every epoch\n",
    "    load_best_model_at_end=True, # reloads the model checkpoint with the best validation metrics\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  accuracy = accuracy_score(labels, predictions)\n",
    "  return {'accuracy': accuracy}\n",
    "\n",
    "# Trainer will automatically detect if a GPU like CUDA is available and if it is, it will use it\n",
    "trainer = Trainer(\n",
    "    model=distilbert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_accuracy,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stops training if validation loss doesn't improve for 3 epochs; prevents overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ZaxQekKyQBmn",
   "metadata": {
    "id": "ZaxQekKyQBmn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14065' max='28130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14065/28130 38:58 < 38:58, 6.01 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>0.199448</td>\n",
       "      <td>0.931933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.192555</td>\n",
       "      <td>0.938067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.208787</td>\n",
       "      <td>0.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.940267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.290175</td>\n",
       "      <td>0.938767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2036733478307724\n",
      "Test accuracy: 0.9353947368421053\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating on the AG dataset\n",
    "# Note: For the full training dataset, this step takes about 15min per epoch\n",
    "\n",
    "# Training the model on the training and validation sets\n",
    "trainer.train()\n",
    "\n",
    "# Final evaluation on the unseen test data\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"Test loss:\", test_metrics['eval_loss'])\n",
    "print(\"Test accuracy:\", test_metrics['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a39d88-66a6-4a7b-abcc-b31ee48719b4",
   "metadata": {},
   "source": [
    "## Final Results\n",
    "The fine-tuned DistilBERT model achieves 93.5% test accuracy on the AG News dataset. Comparsion to traditional and CNN-based approaches is presented in final_results.md."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
